  0%|                                                                                                                                            | 0/104061 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/dang.nh4/PULSE/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation=None)
  File "/home/dang.nh4/PULSE/LLaVA/llava/train/train.py", line 522, in train
    trainer.train()
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/transformers/trainer.py", line 2772, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/transformers/trainer.py", line 2795, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1416, in _pre_forward
    self._sync_buffers()
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 2041, in _sync_buffers
    self._sync_module_buffers(authoritative_rank)
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 2045, in _sync_module_buffers
    self._default_broadcast_coalesced(authoritative_rank=authoritative_rank)
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 2066, in _default_broadcast_coalesced
    self._distributed_broadcast_coalesced(bufs, bucket_size, authoritative_rank)
  File "/home/dang.nh4/miniconda3/envs/ecg-qa/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1982, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 23.59 GiB of which 33.75 MiB is free. Process 3397981 has 16.20 GiB memory in use. Including non-PyTorch memory, this process has 7.34 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 205.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
